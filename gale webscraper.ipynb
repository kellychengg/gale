{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e9bb43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Using /tmp directory due to: [Errno 30] Read-only file system: 'uscis_output'\n",
      "======================================================================\n",
      "USCIS H1B DATA EXTRACTOR\n",
      "======================================================================\n",
      "Target: Fiscal Year 2025, Quarter 3\n",
      "======================================================================\n",
      "\n",
      "[Step 1/5] Fetching USCIS data index page...\n",
      "Fetching: https://www.uscis.gov/tools/reports-and-studies/immigration-and-citizenship-data\n",
      "✓ Status: 200\n",
      "\n",
      "[Step 2/5] Finding FY2025 Q3 page...\n",
      "Searching for FY2025 Q3 page...\n",
      "  → Found candidate: All USCIS Application and Petition Form Types (Fiscal Year 2025, Quart...\n",
      "  → Found candidate: Form I-765, Application for Employment Authorization, Eligibility Cate...\n",
      "  → Found candidate: Form I-765, Application for Employment Authorization Counts of Pending...\n",
      "  → Found candidate: Form I-526, Immigrant Petition by Standalone Investor and Form I-526E,...\n",
      "  → Found candidate: Form I-130, (Awaiting Visa Availability) (Fiscal Year 2025, Quarter 3)...\n",
      "  → Found candidate: Form I-360, Petition for Special Immigrant with a Classification of Sp...\n",
      "  → Found candidate: Form I-140, I-360, I-526 Approved EB Petitions Awaiting Visa Final Pri...\n",
      "  → Found candidate: Form I-907, Request for Premium Processing Service By Underlying Form ...\n",
      "  → Found candidate: Active DACA Recipients – (Fiscal Year 2025, Quarter 3)(XLSX, 55.75 KB)...\n",
      "  → Found candidate: Nonimmigrant Worker Petitions by Case Status and Request for Evidence ...\n",
      "  → Found candidate: Form N‐400, Application for Naturalization, by Category of Naturalizat...\n",
      "  → Found candidate: Form I-918, Petition for U Nonimmigrant Status, by Fiscal Year, Quarte...\n",
      "  → Found candidate: Deferred Action for Childhood Arrivals (DACA) Quarterly Report (Fiscal...\n",
      "  → Found candidate: Form I‐360, Petition for Amerasian, Widow(er), or Special Immigrant, b...\n",
      "  → Found candidate: I‐360 Petitions for Special Immigrant with a Classification of Special...\n",
      "  → Found candidate: I-130, Petition for Alien Relative, by Category, Case Status, and USCI...\n",
      "  → Found candidate: Net Backlog and Frontlog (Fiscal Year 2025, Quarter 3)(XLSX, 324.67 KB...\n",
      "  → Found candidate: Application for Adjustment of Status (Form I-485) Quarterly Report (Fi...\n",
      "  → Found candidate: Form I‐914, Application for T Nonimmigrant Status by Fiscal Year, Quar...\n",
      "  → Found candidate: Form I-140 by Fiscal Year, Quarter and Case Status (Fiscal Year 2025, ...\n",
      "  → Found candidate: Form I-140, Receipts and Current Status by Preference and Country (Fis...\n",
      "  → Found candidate: Form I-821, Application for Temporary Protected Status Receipts, Appro...\n",
      "  → Found candidate: Number of Waivers by Form Number and Case Status (Fiscal Year 2025, Qu...\n",
      "  → Found candidate: Form I-485, Liberian Refugee Immigration Fairness (LRIF) (Fiscal Year ...\n",
      "  → Found candidate: All USCIS Application and Petition Form Types (Fiscal Year 2025, Quart...\n",
      "  → Found candidate: Form I-765, Application for Employment Authorization, Eligibility Cate...\n",
      "  → Found candidate: Form I-765, Application for Employment Authorization Counts of Pending...\n",
      "  → Found candidate: Form I-130, (Awaiting Visa Availability) (Fiscal Year 2025, Quarter 3)...\n",
      "  → Found candidate: Form I-360, Petition for Special Immigrant with a Classification of Sp...\n",
      "  → Found candidate: Form I-140, I-360, I-526 Approved EB Petitions Awaiting Visa Final Pri...\n",
      "✓ Selected: Deferred Action for Childhood Arrivals (DACA) Quarterly Report (Fiscal Year 2025, Quarter 3)(XLSX, 47.11 KB)\n",
      "\n",
      "Using URL: https://www.uscis.gov/sites/default/files/document/data/daca_performancedata_fy2025_q3.xlsx\n",
      "\n",
      "[Step 3/5] Fetching quarterly page and searching for files...\n",
      "Fetching: https://www.uscis.gov/sites/default/files/document/data/daca_performancedata_fy2025_q3.xlsx\n",
      "✓ Status: 200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SEARCHING FOR TARGET FILES\n",
      "======================================================================\n",
      "⚠ Missing files: i140_quarter, i140_preference, all_forms, eb_awaiting\n",
      "These files may not be available for this quarter yet.\n",
      "\n",
      "✗ No target files found. Exiting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "class USCISDataExtractor:\n",
    "    \"\"\"Extract H1B and I-140 data from USCIS quarterly reports\"\"\"\n",
    "    \n",
    "    def __init__(self, fiscal_year=2025, quarter=3, output_dir='./uscis_output'):\n",
    "        self.base_url = \"https://www.uscis.gov\"\n",
    "        self.index_url = \"https://www.uscis.gov/tools/reports-and-studies/immigration-and-citizenship-data\"\n",
    "        self.fiscal_year = fiscal_year\n",
    "        self.quarter = quarter\n",
    "        \n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        })\n",
    "        \n",
    "        # Set up output directory with FY and Quarter\n",
    "        base_output = Path(output_dir)\n",
    "        self.output_dir = base_output / f\"FY{fiscal_year}_Q{quarter}\"\n",
    "        try:\n",
    "            self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "            print(f\"✓ Output directory: {self.output_dir.absolute()}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Using /tmp directory due to: {e}\")\n",
    "            self.output_dir = Path(f'/tmp/uscis_output/FY{fiscal_year}_Q{quarter}')\n",
    "            self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Target file patterns (works across all quarters)\n",
    "        self.target_patterns = {\n",
    "            'all_forms': [\n",
    "                'all uscis application and petition form types',\n",
    "                'all application and petition form types',\n",
    "                'all form types'\n",
    "            ],\n",
    "            'i140_quarter': [\n",
    "                'i-140 by fiscal year, quarter and case status',\n",
    "                'i140 by fiscal year quarter',\n",
    "                'form i-140 by fiscal year'\n",
    "            ],\n",
    "            'i140_preference': [\n",
    "                'i-140, receipts and current status by preference and country',\n",
    "                'i-140 receipts and current status by preference',\n",
    "                'i140 preference country'\n",
    "            ],\n",
    "            'eb_awaiting': [\n",
    "                'approved eb petitions awaiting visa',\n",
    "                'eb petitions awaiting visa',\n",
    "                'i-140, i-360, i-526 approved eb'\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def fetch_page(self, url):\n",
    "        \"\"\"Fetch and parse a webpage\"\"\"\n",
    "        try:\n",
    "            print(f\"Fetching: {url}\")\n",
    "            response = self.session.get(url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            print(f\"✓ Status: {response.status_code}\\n\")\n",
    "            return BeautifulSoup(response.content, 'html.parser')\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error: {e}\\n\")\n",
    "            return None\n",
    "    \n",
    "    def find_quarterly_page(self, soup):\n",
    "        \"\"\"Find the quarterly reports page for specified FY and Quarter\"\"\"\n",
    "        print(f\"Searching for FY{self.fiscal_year} Q{self.quarter} page...\")\n",
    "        \n",
    "        # Search patterns for the target quarter\n",
    "        patterns = [\n",
    "            rf'fiscal\\s+year\\s+{self.fiscal_year}.*quarter\\s+{self.quarter}',\n",
    "            rf'fy\\s*{self.fiscal_year}.*q\\s*{self.quarter}',\n",
    "            rf'{self.fiscal_year}.*q{self.quarter}',\n",
    "        ]\n",
    "        \n",
    "        links = soup.find_all('a', href=True)\n",
    "        candidates = []\n",
    "        \n",
    "        for link in links:\n",
    "            text = link.get_text(strip=True)\n",
    "            href = link['href']\n",
    "            \n",
    "            # Check if link matches any pattern\n",
    "            for pattern in patterns:\n",
    "                if re.search(pattern, text, re.IGNORECASE):\n",
    "                    full_url = urljoin(self.base_url, href)\n",
    "                    candidates.append({\n",
    "                        'text': text,\n",
    "                        'url': full_url,\n",
    "                        'relevance': self._calculate_relevance(text)\n",
    "                    })\n",
    "                    print(f\"  → Found candidate: {text[:70]}...\")\n",
    "                    break\n",
    "        \n",
    "        if not candidates:\n",
    "            print(f\"⚠ Could not find FY{self.fiscal_year} Q{self.quarter} page\")\n",
    "            print(\"Attempting to construct URL...\")\n",
    "            # Try to construct the URL based on common patterns\n",
    "            constructed_url = self._construct_quarterly_url()\n",
    "            return constructed_url\n",
    "        \n",
    "        # Sort by relevance and return best match\n",
    "        candidates.sort(key=lambda x: x['relevance'], reverse=True)\n",
    "        best_match = candidates[0]\n",
    "        print(f\"✓ Selected: {best_match['text']}\\n\")\n",
    "        return best_match['url']\n",
    "    \n",
    "    def _calculate_relevance(self, text):\n",
    "        \"\"\"Calculate relevance score for quarterly page link\"\"\"\n",
    "        score = 0\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Exact matches get higher scores\n",
    "        if f'quarter {self.quarter}' in text_lower or f'q{self.quarter}' in text_lower:\n",
    "            score += 10\n",
    "        if str(self.fiscal_year) in text:\n",
    "            score += 10\n",
    "        if 'data report' in text_lower:\n",
    "            score += 5\n",
    "        if 'quarterly' in text_lower:\n",
    "            score += 3\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def _construct_quarterly_url(self):\n",
    "        \"\"\"Construct quarterly URL based on common USCIS patterns\"\"\"\n",
    "        # Common URL patterns used by USCIS\n",
    "        patterns = [\n",
    "            f\"{self.base_url}/tools/reports-and-studies/immigration-and-citizenship-data/data-set/all-uscis-application-and-petition-form-types-fy{self.fiscal_year}-q{self.quarter}\",\n",
    "            f\"{self.base_url}/tools/reports-and-studies/immigration-and-citizenship-data/quarterly-reports-fy{self.fiscal_year}-q{self.quarter}\",\n",
    "        ]\n",
    "        \n",
    "        print(\"Trying constructed URLs:\")\n",
    "        for url in patterns:\n",
    "            print(f\"  Testing: {url}\")\n",
    "            try:\n",
    "                response = self.session.head(url, timeout=10)\n",
    "                if response.status_code == 200:\n",
    "                    print(f\"  ✓ Found valid URL\\n\")\n",
    "                    return url\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        print(\"  ⚠ Could not find valid URL\\n\")\n",
    "        return patterns[0]  # Return first pattern as fallback\n",
    "    \n",
    "    def find_target_files(self, soup, page_url):\n",
    "        \"\"\"Find the 4 specific Excel files using flexible pattern matching\"\"\"\n",
    "        print(\"=\" * 70)\n",
    "        print(\"SEARCHING FOR TARGET FILES\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        found_files = {}\n",
    "        links = soup.find_all('a', href=True)\n",
    "        \n",
    "        for link in links:\n",
    "            href = link['href']\n",
    "            text = link.get_text(strip=True)\n",
    "            \n",
    "            # Check if it's an Excel file\n",
    "            if not (href.lower().endswith('.xlsx') or href.lower().endswith('.xls')):\n",
    "                continue\n",
    "            \n",
    "            text_lower = text.lower()\n",
    "            \n",
    "            # Check against each target file pattern\n",
    "            for key, patterns in self.target_patterns.items():\n",
    "                if key in found_files:\n",
    "                    continue  # Already found this file\n",
    "                \n",
    "                for pattern in patterns:\n",
    "                    if pattern in text_lower:\n",
    "                        # Additional check: make sure it matches the fiscal year/quarter\n",
    "                        if self._matches_quarter(text):\n",
    "                            full_url = urljoin(page_url, href)\n",
    "                            filename = Path(href).name\n",
    "                            \n",
    "                            found_files[key] = {\n",
    "                                'title': text,\n",
    "                                'url': full_url,\n",
    "                                'filename': filename\n",
    "                            }\n",
    "                            print(f\"✓ Found: {key}\")\n",
    "                            print(f\"  Title: {text[:60]}...\")\n",
    "                            print(f\"  File: {filename}\\n\")\n",
    "                            break\n",
    "        \n",
    "        # Check if we found all files\n",
    "        missing = set(self.target_patterns.keys()) - set(found_files.keys())\n",
    "        if missing:\n",
    "            print(f\"⚠ Missing files: {', '.join(missing)}\")\n",
    "            print(\"These files may not be available for this quarter yet.\\n\")\n",
    "        \n",
    "        return found_files\n",
    "    \n",
    "    def _matches_quarter(self, text):\n",
    "        \"\"\"Check if file title matches the target fiscal year and quarter\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Check for FY and Quarter\n",
    "        fy_patterns = [f'fy{self.fiscal_year}', f'fiscal year {self.fiscal_year}', f'{self.fiscal_year}']\n",
    "        q_patterns = [f'q{self.quarter}', f'quarter {self.quarter}', f'q {self.quarter}']\n",
    "        \n",
    "        has_fy = any(pattern in text_lower for pattern in fy_patterns)\n",
    "        has_q = any(pattern in text_lower for pattern in q_patterns)\n",
    "        \n",
    "        # If both FY and Q are mentioned, they must match\n",
    "        # If only one is mentioned, it should still match\n",
    "        return has_fy and has_q\n",
    "    \n",
    "    def download_file(self, url, filename):\n",
    "        \"\"\"Download a file from URL\"\"\"\n",
    "        try:\n",
    "            print(f\"Downloading: {filename}...\")\n",
    "            response = self.session.get(url, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            downloads_dir = self.output_dir / 'downloads'\n",
    "            downloads_dir.mkdir(exist_ok=True)\n",
    "            filepath = downloads_dir / filename\n",
    "            \n",
    "            with open(filepath, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            \n",
    "            size_kb = len(response.content) / 1024\n",
    "            print(f\"✓ Downloaded: {filename} ({size_kb:.1f} KB)\\n\")\n",
    "            return filepath\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error downloading {filename}: {e}\\n\")\n",
    "            return None\n",
    "    \n",
    "    def extract_rows_16_17(self, filepath):\n",
    "        \"\"\"Extract rows 16 & 17 from All Forms file (I-129 and I-140 data)\"\"\"\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"EXTRACTING ROWS 16-17 FROM: {filepath.name}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        try:\n",
    "            # Read Excel file\n",
    "            xls = pd.ExcelFile(filepath)\n",
    "            print(f\"Available sheets: {xls.sheet_names}\\n\")\n",
    "            \n",
    "            df = pd.read_excel(filepath, sheet_name=0)\n",
    "            print(f\"Total rows: {len(df)}\")\n",
    "            print(f\"Total columns: {len(df.columns)}\")\n",
    "            print(f\"Columns: {list(df.columns)}\\n\")\n",
    "            \n",
    "            # Extract rows 16 & 17 (0-indexed: 15 and 16)\n",
    "            if len(df) >= 17:\n",
    "                rows_16_17 = df.iloc[15:17]\n",
    "                \n",
    "                print(\"ROWS 16-17 DATA:\")\n",
    "                print(\"-\" * 70)\n",
    "                print(rows_16_17.to_string(index=True))\n",
    "                print(\"-\" * 70)\n",
    "                \n",
    "                # Save to CSV\n",
    "                output_file = self.output_dir / f\"rows_16_17_I129_I140.csv\"\n",
    "                rows_16_17.to_csv(output_file, index=True)\n",
    "                print(f\"\\n✓ Saved to: {output_file}\\n\")\n",
    "                \n",
    "                # Display form types\n",
    "                if len(df.columns) > 0:\n",
    "                    form_col = df.columns[0]\n",
    "                    print(\"\\nFORM TYPE DETAILS:\")\n",
    "                    print(\"-\" * 70)\n",
    "                    for idx in [15, 16]:\n",
    "                        if idx < len(df):\n",
    "                            form_type = df.iloc[idx][form_col]\n",
    "                            print(f\"Row {idx+1}: {form_type}\")\n",
    "                    print()\n",
    "                \n",
    "                return rows_16_17\n",
    "            else:\n",
    "                print(f\"⚠ File has only {len(df)} rows (need at least 17)\\n\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error: {e}\\n\")\n",
    "            return None\n",
    "    \n",
    "    def analyze_all_forms(self, filepath):\n",
    "        \"\"\"Search for I-129 and I-140 data in All Forms file\"\"\"\n",
    "        print(\"=\" * 70)\n",
    "        print(\"ANALYZING I-129 AND I-140 DATA\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_excel(filepath, sheet_name=0)\n",
    "            form_col = df.columns[0]\n",
    "            \n",
    "            # Find I-129 rows\n",
    "            i129_mask = df[form_col].astype(str).str.contains('I-129', case=False, na=False)\n",
    "            i129_data = df[i129_mask]\n",
    "            \n",
    "            # Find I-140 rows\n",
    "            i140_mask = df[form_col].astype(str).str.contains('I-140', case=False, na=False)\n",
    "            i140_data = df[i140_mask]\n",
    "            \n",
    "            results = {}\n",
    "            \n",
    "            # Display I-129 data\n",
    "            if not i129_data.empty:\n",
    "                print(\"\\nI-129 (H-1B & NONIMMIGRANT WORKERS):\")\n",
    "                print(\"-\" * 70)\n",
    "                print(i129_data.to_string(index=True))\n",
    "                print()\n",
    "                results['I-129'] = i129_data.to_dict('records')\n",
    "                \n",
    "                # Save separately\n",
    "                i129_file = self.output_dir / 'I129_data.csv'\n",
    "                i129_data.to_csv(i129_file, index=True)\n",
    "                print(f\"✓ Saved I-129 data to: {i129_file}\\n\")\n",
    "            \n",
    "            # Display I-140 data\n",
    "            if not i140_data.empty:\n",
    "                print(\"I-140 (IMMIGRANT WORKERS):\")\n",
    "                print(\"-\" * 70)\n",
    "                print(i140_data.to_string(index=True))\n",
    "                print()\n",
    "                results['I-140'] = i140_data.to_dict('records')\n",
    "                \n",
    "                # Save separately\n",
    "                i140_file = self.output_dir / 'I140_data.csv'\n",
    "                i140_data.to_csv(i140_file, index=True)\n",
    "                print(f\"✓ Saved I-140 data to: {i140_file}\\n\")\n",
    "            \n",
    "            # Save JSON summary\n",
    "            if results:\n",
    "                json_file = self.output_dir / 'I129_I140_summary.json'\n",
    "                with open(json_file, 'w') as f:\n",
    "                    json.dump(results, f, indent=2, default=str)\n",
    "                print(f\"✓ Saved JSON summary to: {json_file}\\n\")\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error: {e}\\n\")\n",
    "            return None\n",
    "    \n",
    "    def process_i140_files(self, downloaded_files):\n",
    "        \"\"\"Process the three I-140 specific files\"\"\"\n",
    "        print(\"=\" * 70)\n",
    "        print(\"PROCESSING I-140 DETAILED REPORTS\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        i140_keys = ['i140_quarter', 'i140_preference', 'eb_awaiting']\n",
    "        \n",
    "        for key in i140_keys:\n",
    "            if key in downloaded_files:\n",
    "                filepath = downloaded_files[key]\n",
    "                print(f\"\\nProcessing: {filepath.name}\")\n",
    "                print(\"-\" * 70)\n",
    "                \n",
    "                try:\n",
    "                    df = pd.read_excel(filepath, sheet_name=0)\n",
    "                    print(f\"Rows: {len(df)}, Columns: {len(df.columns)}\")\n",
    "                    print(\"\\nFirst 10 rows:\")\n",
    "                    print(df.head(10).to_string())\n",
    "                    print()\n",
    "                    \n",
    "                    # Save as CSV for easier viewing\n",
    "                    csv_file = self.output_dir / f\"{filepath.stem}.csv\"\n",
    "                    df.to_csv(csv_file, index=False)\n",
    "                    print(f\"✓ Saved to: {csv_file}\\n\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"✗ Error: {e}\\n\")\n",
    "    \n",
    "    def run_extraction(self):\n",
    "        \"\"\"Main extraction workflow\"\"\"\n",
    "        print(\"=\" * 70)\n",
    "        print(\"USCIS H1B DATA EXTRACTOR\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"Target: Fiscal Year {self.fiscal_year}, Quarter {self.quarter}\")\n",
    "        print(\"=\" * 70)\n",
    "        print()\n",
    "        \n",
    "        # Step 1: Fetch the index page\n",
    "        print(\"[Step 1/5] Fetching USCIS data index page...\")\n",
    "        soup = self.fetch_page(self.index_url)\n",
    "        \n",
    "        if not soup:\n",
    "            print(\"✗ Failed to fetch index page. Exiting.\\n\")\n",
    "            return\n",
    "        \n",
    "        # Step 2: Find quarterly page\n",
    "        print(f\"[Step 2/5] Finding FY{self.fiscal_year} Q{self.quarter} page...\")\n",
    "        quarterly_url = self.find_quarterly_page(soup)\n",
    "        \n",
    "        if not quarterly_url:\n",
    "            print(\"✗ Could not locate quarterly page. Exiting.\\n\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Using URL: {quarterly_url}\\n\")\n",
    "        \n",
    "        # Step 3: Fetch quarterly page and find files\n",
    "        print(\"[Step 3/5] Fetching quarterly page and searching for files...\")\n",
    "        quarterly_soup = self.fetch_page(quarterly_url)\n",
    "        \n",
    "        if not quarterly_soup:\n",
    "            print(\"✗ Failed to fetch quarterly page. Exiting.\\n\")\n",
    "            return\n",
    "        \n",
    "        found_files = self.find_target_files(quarterly_soup, quarterly_url)\n",
    "        \n",
    "        if not found_files:\n",
    "            print(\"✗ No target files found. Exiting.\\n\")\n",
    "            return\n",
    "        \n",
    "        print(f\"✓ Found {len(found_files)}/4 target files\\n\")\n",
    "        \n",
    "        # Step 4: Download files\n",
    "        print(\"[Step 4/5] Downloading files...\")\n",
    "        print(\"=\" * 70)\n",
    "        downloaded_files = {}\n",
    "        \n",
    "        for key, file_info in found_files.items():\n",
    "            filepath = self.download_file(file_info['url'], file_info['filename'])\n",
    "            if filepath:\n",
    "                downloaded_files[key] = filepath\n",
    "            time.sleep(0.5)  # Be polite to the server\n",
    "        \n",
    "        if not downloaded_files:\n",
    "            print(\"✗ No files were downloaded. Exiting.\\n\")\n",
    "            return\n",
    "        \n",
    "        print(f\"✓ Successfully downloaded {len(downloaded_files)} files\\n\")\n",
    "        \n",
    "        # Step 5: Process files\n",
    "        print(\"[Step 5/5] Processing downloaded files...\")\n",
    "        \n",
    "        # Process \"All Forms\" file\n",
    "        if 'all_forms' in downloaded_files:\n",
    "            print(\"\\n\" + \"=\" * 70)\n",
    "            print(\"PROCESSING: ALL FORMS FILE\")\n",
    "            print(\"=\" * 70)\n",
    "            \n",
    "            all_forms_path = downloaded_files['all_forms']\n",
    "            \n",
    "            # Extract rows 16-17\n",
    "            self.extract_rows_16_17(all_forms_path)\n",
    "            \n",
    "            # Analyze and extract I-129 and I-140 data\n",
    "            self.analyze_all_forms(all_forms_path)\n",
    "        \n",
    "        # Process I-140 specific files\n",
    "        self.process_i140_files(downloaded_files)\n",
    "        \n",
    "        # Final summary\n",
    "        print(\"=\" * 70)\n",
    "        print(\"EXTRACTION COMPLETE!\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"\\nFiscal Year: {self.fiscal_year}, Quarter: {self.quarter}\")\n",
    "        print(f\"Output location: {self.output_dir.absolute()}\")\n",
    "        print(f\"\\nFiles created:\")\n",
    "        print(f\"  📁 downloads/ ({len(downloaded_files)} Excel files)\")\n",
    "        print(f\"  📄 rows_16_17_I129_I140.csv\")\n",
    "        print(f\"  📄 I129_data.csv\")\n",
    "        print(f\"  📄 I140_data.csv\")\n",
    "        print(f\"  📄 I129_I140_summary.json\")\n",
    "        print(f\"  📄 Processed I-140 CSV reports\")\n",
    "        print()\n",
    "\n",
    "def extract_multiple_quarters(quarters_list, output_dir='./uscis_output'):\n",
    "    \"\"\"\n",
    "    Extract data for multiple quarters\n",
    "    \n",
    "    Args:\n",
    "        quarters_list: List of tuples [(fiscal_year, quarter), ...]\n",
    "                      Example: [(2025, 3), (2025, 2), (2024, 4)]\n",
    "        output_dir: Base output directory\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"MULTI-QUARTER EXTRACTION\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Extracting data for {len(quarters_list)} quarters\\n\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for fy, q in quarters_list:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Starting extraction for FY{fy} Q{q}\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        extractor = USCISDataExtractor(\n",
    "            fiscal_year=fy,\n",
    "            quarter=q,\n",
    "            output_dir=output_dir\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            extractor.run_extraction()\n",
    "            results.append({\n",
    "                'fiscal_year': fy,\n",
    "                'quarter': q,\n",
    "                'status': 'success',\n",
    "                'output_dir': str(extractor.output_dir)\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error extracting FY{fy} Q{q}: {e}\\n\")\n",
    "            results.append({\n",
    "                'fiscal_year': fy,\n",
    "                'quarter': q,\n",
    "                'status': 'failed',\n",
    "                'error': str(e)\n",
    "            })\n",
    "        \n",
    "        time.sleep(2)  # Wait between quarters\n",
    "    \n",
    "    # Save summary\n",
    "    summary_path = Path(output_dir) / 'extraction_summary.json'\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"MULTI-QUARTER EXTRACTION COMPLETE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Summary saved to: {summary_path}\")\n",
    "    print(f\"\\nResults:\")\n",
    "    for r in results:\n",
    "        status_icon = \"✓\" if r['status'] == 'success' else \"✗\"\n",
    "        print(f\"  {status_icon} FY{r['fiscal_year']} Q{r['quarter']}: {r['status']}\")\n",
    "    print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    \n",
    "    if len(sys.argv) >= 3:\n",
    "        # Command line: python script.py <fiscal_year> <quarter> [output_dir]\n",
    "        fy = int(sys.argv[1])\n",
    "        q = int(sys.argv[2])\n",
    "        output = sys.argv[3] if len(sys.argv) > 3 else './uscis_output'\n",
    "        \n",
    "        extractor = USCISDataExtractor(fiscal_year=fy, quarter=q, output_dir=output)\n",
    "        extractor.run_extraction()\n",
    "    \n",
    "    elif len(sys.argv) == 2 and sys.argv[1] == '--multi':\n",
    "        # Multi-quarter mode\n",
    "        # Extract last 4 quarters as example\n",
    "        current_fy = 2025\n",
    "        quarters = [\n",
    "            (2025, 3),\n",
    "            (2025, 2),\n",
    "            (2025, 1),\n",
    "            (2024, 4),\n",
    "        ]\n",
    "        extract_multiple_quarters(quarters)\n",
    "    \n",
    "    else:\n",
    "        # Default: FY2025 Q3\n",
    "        extractor = USCISDataExtractor(fiscal_year=2025, quarter=3)\n",
    "        extractor.run_extraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c8b180",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
