# USCIS Data Scrapers  
This repository contains two automated Python scrapers that collect, deduplicate, and organize public datasets from the **U.S. Citizenship and Immigration Services (USCIS)**. The first scraper focuses on **H-1B, H-2A, and H-2B employer data hub files**, while the second scraper dynamically crawls the USCIS data library for **I-140, I-129, I-765, I-907, I-485, and EB petition datasets**.  

## Project Structure  
uscis_data/  
â”œâ”€â”€ h1b/                  # H-1B employer data files  
â”œâ”€â”€ h2a/                  # H-2A employer data files  
â”œâ”€â”€ h2b/                  # H-2B employer data files  
â”œâ”€â”€ I-140/                # I-140 (Immigrant Petition for Alien Worker)  
â”œâ”€â”€ I-129/                # I-129 (Nonimmigrant Worker)  
â”œâ”€â”€ I-765/                # I-765 (Employment Authorization)  
â”œâ”€â”€ I-907/                # I-907 (Premium Processing)  
â”œâ”€â”€ I-485/                # I-485 (Adjustment of Status)  
â”œâ”€â”€ EB/                   # Employment-Based Petitions  
â”œâ”€â”€ logs/                 # Rotating logs (auto-cleaned after 90 days)  
â”œâ”€â”€ metadata.json         # File metadata (for Data Hub scraper)  
â”œâ”€â”€ checksums.json        # SHA-256 checksums for deduplication  
â”œâ”€â”€ download_manifest.json # Manifest for Immigration Forms scraper  
â””â”€â”€ report_YYYYMMDD.txt   # Generated reports  

## USCIS Data Hub Scraper  
**File:** `uscis_data_hub_scraper.py` â€” Automates downloads of H-1B, H-2A, and H-2B employer data from USCISâ€™s archived data hub pages. It uses checksum-based deduplication, metadata tracking, retry logic with exponential backoff, and automatic scheduling.  
**Features:** Scrapes all visa hub pages automatically â€¢ Deduplication via SHA-256 â€¢ Metadata tracking of filenames, timestamps, and sizes â€¢ Retry logic (5 attempts, exponential backoff) â€¢ Log cleanup after 90 days â€¢ Monthly scheduling at 2:00 AM on the 1st â€¢ Writes `SCRAPE_FAILURE.txt` if all retries fail.  
**Run Manually:** `python uscis_data_hub_scraper.py`  
**Automatic Schedule:** `schedule.every().month.at("02:00").do(scheduled_job_with_retry, download_dir)`  
**Example Log:**  
2025-11-06 02:00:00 â€“ INFO â€“ Scraping H1B dataâ€¦  
2025-11-06 02:01:10 â€“ INFO â€“ Successfully downloaded H-1B_FY2024_Q3.xlsx  
2025-11-06 02:01:11 â€“ INFO â€“ Metadata saved successfully  

## ğŸ§¾ USCIS Immigration Forms Data Scraper  
**File:** `uscis_forms_scraper.py` â€” Crawls the USCIS â€œReports and Studiesâ€ data library to find and download datasets for I-140, I-129, I-765, I-907, I-485, and EB petitions. It paginates through hundreds of pages, matches keywords, and saves results into organized subfolders.  
**Features:** Dynamic discovery â€¢ Keyword-based form matching â€¢ Organized subfolders per form â€¢ Manifest tracking â€¢ Duplicate detection via MD5 â€¢ Configurable page limits â€¢ Graceful recovery from network errors.  
**Run Manually:** `python uscis_forms_scraper.py`  
**Test Mode:** `scraper.run(max_pages=10, delay_between_downloads=1.0)`  
**Target Forms:** I-140 (Alien Worker) â€¢ I-129 (Nonimmigrant Worker â€“ H-1B/L-1/O-1/TN) â€¢ I-765 (EAD/OPT/STEM OPT) â€¢ I-907 (Premium Processing) â€¢ I-485 (Adjustment of Status) â€¢ EB (Employment-Based Petitions).  

## Installation  
`pip install requests beautifulsoup4 schedule`  
Optional: `pip install python-crontab`  

## Outputs  
Each run produces downloaded files organized by visa/form type, manifest and metadata JSONs, logs under `/logs`, and report text files summarizing file counts and sizes. 

## Maintenance  
Logs older than 90 days are deleted â€¢ Missing/corrupted files are detected and fixed â€¢ Repeated failures trigger `SCRAPE_FAILURE.txt`.  

## Recommended Layout  
project_root/  
â”œâ”€â”€ uscis_data_hub_scraper.py  
â”œâ”€â”€ uscis_forms_scraper.py  
â”œâ”€â”€ requirements.txt  
â”œâ”€â”€ README.md  
â””â”€â”€ uscis_data/  


